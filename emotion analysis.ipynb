{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GolahXTUep3u"
      },
      "outputs": [],
      "source": [
        "import numpy as np # linear algebra\n",
        "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "# Word Cloud\n",
        "from wordcloud import WordCloud\n",
        "# from textacy import preprocessing\n",
        "from nltk.stem.snowball import SnowballStemmer\n",
        "from keras.preprocessing import sequence\n",
        "from sklearn.model_selection import train_test_split\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import *\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "import re"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "s8r46wMSep7h"
      },
      "outputs": [],
      "source": [
        "# Load Data\n",
        "df = pd.read_csv('text.csv')\n",
        "df.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "r1fLqUQleqB_"
      },
      "outputs": [],
      "source": [
        "# Rename Columns\n",
        "df.rename(columns={'text': 'Text', 'label': 'Label'}, inplace=True)\n",
        "# Dropping the Index Colums\n",
        "df.drop('Unnamed: 0',axis=1,inplace=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5YZGZ_5reqIB"
      },
      "outputs": [],
      "source": [
        "# Lets Rename Label also {0: 'sadness', 1: 'joy', 2: 'love', 3: 'anger', 4: 'fear', 5: 'surprise'}\n",
        "df['Label'] = df['Label'].replace(0,'Sadness')\n",
        "df['Label'] = df['Label'].replace(1,'Joy')\n",
        "df['Label'] = df['Label'].replace(2,'Love')\n",
        "df['Label'] = df['Label'].replace(3,'Anger')\n",
        "df['Label'] = df['Label'].replace(4,'Fear')\n",
        "df['Label'] = df['Label'].replace(5,'Surprise')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pT9hEvscfGbQ"
      },
      "outputs": [],
      "source": [
        "# Value Count Of Label\n",
        "count = df['Label'].value_counts()\n",
        "\n",
        "# Create a figure with two subplots\n",
        "fig, axs = plt.subplots(1, 2, figsize=(12, 6), facecolor='white')\n",
        "\n",
        "# Plot pie chart on the first subplot\n",
        "palette = sns.color_palette(\"viridis\")\n",
        "sns.set_palette(palette)\n",
        "axs[0].pie(count, labels=count.index, autopct='%1.1f%%', startangle=140)\n",
        "axs[0].set_title('Distribution of Categories')\n",
        "\n",
        "# Plot bar chart on the second subplot\n",
        "sns.barplot(x=count.index, y=count.values, ax=axs[1], palette=\"viridis\")\n",
        "axs[1].set_title('Count of Categories')\n",
        "\n",
        "# Adjust layout\n",
        "plt.tight_layout()\n",
        "\n",
        "# Show the plot\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MoPEtHTMfGXp"
      },
      "outputs": [],
      "source": [
        "# Make Seperate Data Set to Visualize text\n",
        "# Sadness\n",
        "df_sadness = df[df['Label']=='Sadness']\n",
        "# Joy\n",
        "df_joy = df[df['Label']=='Joy']\n",
        "# Love\n",
        "df_love = df[df['Label']=='Love']\n",
        "# Anger\n",
        "df_anger = df[df['Label']=='Anger']\n",
        "# Fear\n",
        "df_fear = df[df['Label']=='Fear']\n",
        "# Surprise\n",
        "df_surprise = df[df['Label']=='Surprise']"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IP30Mn7HfGVh"
      },
      "outputs": [],
      "source": [
        "# Combine text from different categories\n",
        "combined_sadness_text = ' '.join(df_sadness['Text'])\n",
        "combined_joy_text = ' '.join(df_joy['Text'])\n",
        "combined_love_text = ' '.join(df_love['Text'])\n",
        "combined_anger_text = ' '.join(df_anger['Text'])\n",
        "combined_fear_text = ' '.join(df_fear['Text'])\n",
        "combined_surprise_text = ' '.join(df_surprise['Text'])\n",
        "\n",
        "# Create word clouds\n",
        "sadness_wordcloud = WordCloud(width=800, height=400, background_color='white').generate(combined_sadness_text)\n",
        "joy_wordcloud = WordCloud(width=800, height=400, background_color='white').generate(combined_joy_text)\n",
        "love_wordcloud = WordCloud(width=800, height=400, background_color='white').generate(combined_love_text)\n",
        "anger_wordcloud = WordCloud(width=800, height=400, background_color='white').generate(combined_anger_text)\n",
        "fear_wordcloud = WordCloud(width=800, height=400, background_color='white').generate(combined_fear_text)\n",
        "surprise_wordcloud = WordCloud(width=800, height=400, background_color='white').generate(combined_surprise_text)\n",
        "\n",
        "# Plot the word clouds\n",
        "plt.figure(figsize=(18, 9))\n",
        "\n",
        "plt.subplot(2, 3, 1)\n",
        "plt.imshow(sadness_wordcloud, interpolation='bilinear')\n",
        "plt.title('Sadness Text')\n",
        "plt.axis('off')\n",
        "\n",
        "plt.subplot(2, 3, 2)\n",
        "plt.imshow(joy_wordcloud, interpolation='bilinear')\n",
        "plt.title('Joy Text')\n",
        "plt.axis('off')\n",
        "\n",
        "plt.subplot(2, 3, 3)\n",
        "plt.imshow(love_wordcloud, interpolation='bilinear')\n",
        "plt.title('Love Text')\n",
        "plt.axis('off')\n",
        "\n",
        "plt.subplot(2, 3, 4)\n",
        "plt.imshow(anger_wordcloud, interpolation='bilinear')\n",
        "plt.title('Anger Text')\n",
        "plt.axis('off')\n",
        "\n",
        "plt.subplot(2, 3, 5)\n",
        "plt.imshow(fear_wordcloud, interpolation='bilinear')\n",
        "plt.title('Fear Text')\n",
        "plt.axis('off')\n",
        "\n",
        "plt.subplot(2, 3, 6)\n",
        "plt.imshow(surprise_wordcloud, interpolation='bilinear')\n",
        "plt.title('Surprise Text')\n",
        "plt.axis('off')\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SMnF8j7wfGTr"
      },
      "outputs": [],
      "source": [
        "df['Label'] = df['Label'].replace('Sadness',0)\n",
        "df['Label'] = df['Label'].replace('Joy',1)\n",
        "df['Label'] = df['Label'].replace('Love',2)\n",
        "df['Label'] = df['Label'].replace('Anger',3)\n",
        "df['Label'] = df['Label'].replace('Fear',4)\n",
        "df['Label'] = df['Label'].replace('Surprise',5)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MRCM0WRKfekO"
      },
      "outputs": [],
      "source": [
        "# Import Basis Needed Libaries\n",
        "import nltk\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.stem import PorterStemmer\n",
        "\n",
        "# Download NLTK resources (uncomment the following line if not already downloaded)?\n",
        "nltk.download('punkt')\n",
        "nltk.download('stopwords')\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yiKgmcQpfegw"
      },
      "outputs": [],
      "source": [
        "# Step 1: Remove URLs\n",
        "df['Text'] = df['Text'].str.replace(r'http\\S+', '', regex=True)\n",
        "\n",
        "# Display the first few rows of the DataFrame to verify the changes\n",
        "print(df.head())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Vix5bBUTfeek"
      },
      "outputs": [],
      "source": [
        "# Step 2: Remove special characters and punctuation\n",
        "df['Text'] = df['Text'].str.replace(r'[^\\w\\s]', '', regex=True)\n",
        "\n",
        "# Display the first few rows of the DataFrame to verify the changes\n",
        "print(df.head())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Ki0Jtn92febF"
      },
      "outputs": [],
      "source": [
        "# Step 3: Remove extra whitespaces\n",
        "df['Text'] = df['Text'].str.replace(r'\\s+', ' ', regex=True)\n",
        "\n",
        "# Display the first few rows of the DataFrame to verify the changes\n",
        "print(df.head())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WZcOLs-ffr2D"
      },
      "outputs": [],
      "source": [
        "# Step : 4 Remove numeric values\n",
        "df['Text'] = df['Text'].str.replace(r'\\d+', '', regex=True)\n",
        "\n",
        "# Head\n",
        "df.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DMW8Vp7gfryH"
      },
      "outputs": [],
      "source": [
        "# Step 6: Lowercasing\n",
        "df['Text'] = df['Text'].str.lower()\n",
        "\n",
        "# Display the first few rows of the DataFrame to verify the changes\n",
        "print(df.head())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XT8AczwFfrv5"
      },
      "outputs": [],
      "source": [
        "# Step 8: Remove stop words\n",
        "stop = stopwords.words('english')\n",
        "df[\"Text\"] = df['Text'].apply(lambda x: ' '.join([word for word in x.split() if word not in (stop)]))\n",
        "\n",
        "# Display the first few rows of the DataFrame to verify the changes\n",
        "df.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vSlkegTLfruA"
      },
      "outputs": [],
      "source": [
        "# Step : 9 Remove non-alphanumeric characters from the 'Text' column\n",
        "df['Text'] = df['Text'].apply(lambda x: re.sub(r'[^a-zA-Z\\s]', '', x))\n",
        "\n",
        "# Display the first few rows of the DataFrame to verify the changes\n",
        "df.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xpWVTA1Sf08F"
      },
      "outputs": [],
      "source": [
        "X = df['Text']\n",
        "y = df['Label']\n",
        "\n",
        "# Train Test Split\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Print the shapes of the training and testing sets\n",
        "print(\"X_train shape:\", X_train.shape)\n",
        "print(\"X_test shape:\", X_test.shape)\n",
        "print(\"y_train shape:\", y_train.shape)\n",
        "print(\"y_test shape:\", y_test.shape)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zGn5ueXgf05Q"
      },
      "outputs": [],
      "source": [
        "# Tokenize the text data\n",
        "tokenizer = Tokenizer(num_words=50000)\n",
        "tokenizer.fit_on_texts(X_train)\n",
        "tokenizer.fit_on_texts(X_test)\n",
        "X_train_sequences = tokenizer.texts_to_sequences(X_train)\n",
        "X_test_sequences = tokenizer.texts_to_sequences(X_test)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1La3tflgf018"
      },
      "outputs": [],
      "source": [
        "# Max Len in X_train_sequences\n",
        "maxlen = max(len(tokens) for tokens in X_train_sequences)\n",
        "print(\"Maximum sequence length (maxlen):\", maxlen)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "27_N6BYif0y8"
      },
      "outputs": [],
      "source": [
        "# Perform padding on X_train and X_test sequences\n",
        "X_train_padded = pad_sequences(X_train_sequences, maxlen=maxlen, padding='post',)\n",
        "X_test_padded = pad_sequences(X_test_sequences, maxlen=maxlen, padding='post')\n",
        "\n",
        "# Print the padded sequences for X_train and X_test\n",
        "print(\"X_train_padded:\")\n",
        "print(X_train_padded)\n",
        "print(\"\\nX_test_padded:\")\n",
        "print(X_test_padded)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hA7JK_bBgFn-"
      },
      "outputs": [],
      "source": [
        "# Define the model\n",
        "model = Sequential()\n",
        "\n",
        "# Add embedding layer\n",
        "model.add(Embedding(input_dim=input_Size, output_dim=50, input_length=maxlen))\n",
        "\n",
        "# Dropout\n",
        "model.add(Dropout(0.5))\n",
        "\n",
        "# Add Bidirectional LSTM layer\n",
        "model.add(Bidirectional(GRU(120, return_sequences=True)))\n",
        "model.add(Bidirectional(GRU(64, return_sequences=True)))\n",
        "\n",
        "#Batch Normalization\n",
        "model.add(BatchNormalization())\n",
        "\n",
        "# Add Bidirectional GRU layer\n",
        "model.add(Bidirectional(GRU(64)))\n",
        "\n",
        "# Add output layer\n",
        "model.add(Dense(6, activation='softmax'))\n",
        "\n",
        "# Compile the model\n",
        "model.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
        "\n",
        "# Print model summary\n",
        "model.summary()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Gr1St9rggFln"
      },
      "outputs": [],
      "source": [
        "# Model Train\n",
        "history = model.fit(X_train_padded, y_train, epochs=5, batch_size=1500, validation_data=(X_test_padded, y_test))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7nnTypg3gFiT"
      },
      "outputs": [],
      "source": [
        "# Get the epoch with the highest validation accuracy\n",
        "best_epoch = history.history['val_accuracy'].index(max(history.history['val_accuracy'])) + 1\n",
        "\n",
        "# Create a subplot with 1 row and 2 columns\n",
        "fig, axs = plt.subplots(1, 2, figsize=(16, 5))\n",
        "\n",
        "# Plot training and validation accuracy\n",
        "axs[0].plot(history.history['accuracy'], label='Training Accuracy', color='blue')\n",
        "axs[0].plot(history.history['val_accuracy'], label='Validation Accuracy', color='red')\n",
        "axs[0].scatter(best_epoch - 1, history.history['val_accuracy'][best_epoch - 1], color='green', label=f'Best Epoch: {best_epoch}')\n",
        "axs[0].set_xlabel('Epoch')\n",
        "axs[0].set_ylabel('Accuracy')\n",
        "axs[0].set_title('Training and Validation Accuracy')\n",
        "axs[0].legend()\n",
        "\n",
        "\n",
        "# Plot training and validation loss\n",
        "axs[1].plot(history.history['loss'], label='Training Loss', color='blue')\n",
        "axs[1].plot(history.history['val_loss'], label='Validation Loss', color='red')\n",
        "axs[1].scatter(best_epoch - 1, history.history['val_loss'][best_epoch - 1], color='green',label=f'Best Epoch: {best_epoch}')\n",
        "axs[1].set_xlabel('Epoch')\n",
        "axs[1].set_ylabel('Loss')\n",
        "axs[1].set_title('Training and Validation Loss')\n",
        "axs[1].legend()\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ]
    }
  ],
  "metadata": {
    "accelerator": "TPU",
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
